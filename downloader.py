"""
Downloads user specified amount of images

Production use case:
    python download.py --limit 1000000000000 --source /path/to/wiki-parquet/ --output /path/to/save/images

    s3://wiki/plainstopeaks/20210120/batch_1/assets/0/0/0/1/00001121215151/1/image.jpeg
    s3://wiki/plainstopeaks/20210120/batch_1/assets/0/0/0/1/00001121215151/2/image.jpeg
    s3://wiki/plainstopeaks/20210120/batch_1/assets/0/0/0/1/00001121215151/3/image.jpeg
    s3://wiki/plainstopeaks/20210120/batch_1/data/
"""
import os
import sys
import time
import getopt
import logging
import pandas as pd
from wikiutils.utils import Utils

if __name__ == "__main__":
    pass

# Helper classes
utils = Utils()

# Input parameters
partner_name = ""  # Partner name
download_limit = 0  # Total number of bytes to download
batch_size = 0  # Size of download batches (in bytes)
input_df = ""  # input of parquet files generated by ingestion3 wiki job
base_output_path = ""  # output location
max_filesize = 10737418240  # Default max file size is 10GB  -- We can probably ignore this parameter from now on
file_filter = ""

# Controlling vars
df_rows = list()  #
total_downloaded = 0  # Running incrementer for tracking total bytes downloaded
batch_downloaded = 0  # Running incrementer for tracking total bytes downloaded in a batch
batch_number = 1  # This will break apart the input parquet file into batches defined by batch_size

# index and column names for the input parquet file
columns = {"_1": "id", 
           "_2": "wiki_markup", 
           "_3": "iiif", 
           "_4": "media_master", 
           "_5": "title"}

# column names for the output parquet file
upload_parquet_columns = ['dpla_id', 'path', 'size', 'title', 'markup', 'page']

try:
    opts, args = getopt.getopt(sys.argv[1:],
                               "hi:u:o:", 
                               ["partner=",
                                "limit=", 
                                "batch_size=", 
                                "input=", 
                                "output=", 
                                "max_filesize=", 
                                "file_filter="])
except getopt.GetoptError:
    print(
        "downloader.py\n" \
        "--partner <dpla partner name>\n" \
        "--limit <bytes>\n" \
        "--batch_size <bytes>\n" \
        "--input <path to parquet>\n" \
        "--output <path to save files>\n" \
        "--max_filesize <max filesize>\n" \
        "--file_filter <ids>" \
        )
    sys.exit(2)

for opt, arg in opts:
    if opt == '-h':
        print(
            "downloader.py\n" \
                "--partner <dpla partner name>\n" \
                "--limit <total limit in bytes>\n" \
                "--batch_size <batch size limit in bytes>\n" \
                "--input <path to wikimedia parquet file>\n" \
                "--output <path to save files>\n" \
                "--max_filesize <max file size in bytes>" \
                "--file_filter <file that specifies DPLA ids to download>"
                )
        sys.exit()
    elif opt in ("-p", "--partner"):
        partner_name = arg
    elif opt in ("-l", "--limit"):
        download_limit = int(arg)
    elif opt in ("-i", "--input"):
        input_df = arg
    elif opt in ("-o", "--output"):
        base_output_path = arg.rstrip('/')
    elif opt in ("-b", "--batch_size"):
        batch_size = int(arg)
    elif opt in ("-m", "--max_filesize"):
        max_filesize = int(arg)
    elif opt in ("-f", "--file_filter"):
        file_filter = arg

# Setup logging configuration

DATE_TIME = time.strftime("%Y%m%d-%H%M%S")
log_file = f"logs/{partner_name}-download-{DATE_TIME}.log"
os.makedirs(os.path.dirname(log_file), exist_ok=True)

logging.basicConfig(format="[%(levelname)s] %(asctime)s: %(message)s",
                    level=logging.INFO, 
                    datefmt="%H:%M:%S", 
                    handlers=[logging.StreamHandler(), 
                              logging.FileHandler(log_file, mode="w")] 
                    )
logger = logging.getLogger('logger')

# Summary of input parameters
logger.info("Total download limit: %s", utils.sizeof_fmt(download_limit))  
logger.info("Batch size: %s", utils.sizeof_fmt(batch_size))
logger.info("Max file size: %s", utils.sizeof_fmt(max_filesize))
logger.info("Input: %s", input_df)
logger.info("Output: %s", base_output_path)

# If using file filter then read in the file and create a list of DPLA IDs
ids = list()
if file_filter:
    logger.info("Using filter: %s", file_filter)
    with open(file_filter, encoding='utf-8') as f:
        ids = [line.rstrip() for line in f]
    logger.info("Attempting %s DPLA records", len(ids))

# Get individual parquet files from ingestion3 wiki output
file_list = utils.get_parquet_files(path=input_df)

# Path to save the dataframe which holds all the metadata for the batch of downloaded files
df_batch_out = f"{base_output_path}/batch_{batch_number}/data/"
dpla_item_count = 1

# TODO rewrite this to use a generator
for parquet_file in file_list:
    utils.create_path(df_batch_out)
    df = utils.get_df(parquet_file, columns=columns)

    # TODO rewrite this to use a generator
    for row in df.itertuples(index=['id', 'wiki_markup', 'iiif', 'media_master', 'title']):
        try:
            dpla_id = getattr(row, 'id')
            title = getattr(row, 'title')
            wiki_markup = getattr(row, 'wiki_markup')
            iiif = getattr(row, 'iiif')
            media_master = getattr(row, 'media_master')
        except Exception as e:
            logger.error(f"Unable to get attributes from row {row}: {e}. Aborting...")
            break

        # If a file_filter paramter is specified then only download files that match the DPLA IDs in 
        # the file
        # TODO Rewrite this as a prefilter of the parquet files
        if file_filter and (dpla_id not in ids):
            continue

        # Are we working with IIIF or media_master?
        download_urls = utils.get_iiif_urls(iiif) if iiif else media_master

        """
        Get urls to download
        Download images up to 1TB
        Store them and then upload later

        Generate values required by uploader
        upload parquet file
            - dpla id
            - Page title
            - Path to asset
            - size of asset
            - Wiki markup
        """
        # TODO `out` should be the root save location or the asset path?
        out, time, size = base_output_path, 0, 0  # Defaults
        asset_count = 1

        # MULTI-ASSET SUPPORT
        rows = list()
        logger.info("https://dp.la/item/%s has %s assets", dpla_id, len(download_urls))
        for url in download_urls:
            # Creates the destination path for the asset
            destination_path = utils.create_destination_path(base_output_path, batch_number, asset_count, dpla_id) 
            # If the destination path is not an S3 path then create the path on the local file system
            if not base_output_path.startswith("s3://"):
                utils.create_path(destination_path.lstrip('/'))
            
            try:
                out, size = utils.download(source=url, destination=destination_path)
            except Exception as e:
                # If a single asset fails for a multi-asset upload then all assets are dropped
                logger.error("Aborting all  assets for %s\n- %sn", dpla_id, str(e))
                rows = list()
                out = None
                time, size = 0, 0
                break
            
            if out is None: 
                logger.info("Asset %s failed to download. Skipping...", url)
                break

            row = {
                'dpla_id': dpla_id,
                'path': out,
                'size': size,
                'title': title,
                'markup': wiki_markup,
                'page': asset_count
            }
            rows.append(row)

            asset_count =+ 1  # increment asset count
            batch_downloaded =+ size # track the cumluative size of this batch
            total_downloaded =+ size # track the cumluative size of the total download

        # append all assets/rows for a given metadata record
        if len(rows) > 0:
            df_rows.extend(rows)

        dpla_item_count =+ 1

        if batch_downloaded > batch_size:
            logger.info("Download quota met for batch %s", batch_number)
            logger.info(f"\n\tBatch {batch_number} \n" \
                            f"\t{len(df_rows)} files \n" \
                            f"\t{utils.sizeof_fmt(batch_downloaded)}")

            # Save upload info dataframe
            batch_parquet_out_path = f"{df_batch_out}batch_{batch_number}.parquet"
            utils.write_parquet(batch_parquet_out_path, df_rows, upload_parquet_columns)

            # Reset batch control vars, update df_output_path
            df_rows = list()
            batch_number = batch_number + 1
            batch_downloaded = 0
            df_batch_out = f"{base_output_path}/batch_{batch_number}/data/"

        # If there is a total limit in place then abort after it has been breached.
        if 0 < download_limit < total_downloaded:
            logger.info(f"Total download limit reacbed %s", {utils.sizeof_fmt(total_downloaded)})
            batch_parquet_out_path = f"{df_batch_out}batch_{batch_number}.parquet"
            utils.write_parquet(batch_parquet_out_path, df_rows, upload_parquet_columns)
            df_rows = list()

# If finished processing parquet files without breaching limits then write data out
if df_rows:
    batch_parquet_out_path = f"{df_batch_out}batch_{batch_number}.parquet"
    utils.write_parquet(batch_parquet_out_path, df_rows, upload_parquet_columns)

# TODO Fix this and have it write out for all records rather than just the last one. :/ 
# write a summary of the images downloaded
# input_df = pd.DataFrame({   'dpla_id': [dpla_id],
#                             'title': [title],
#                             'wiki_markup': [wiki_markup],
#                             'iiif': [iiif],
#                             'media_master': [media_master],
#                             'downloaded': [out],
#                             'download_time': [time],
#                             'download_size': [size]
#                         })
# input_df.to_parquet(f"{base_output_path}/input.parquet")

logger.info("FINISHED download for %s", input_df)

bucket, key = utils.get_bucket_key(input)
with open(log_file, "rb") as f:
    utils.upload_to_s3(file=f, bucket=bucket, key=f"{key}logs/{log_file}", content_type="text/plain")